{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Maze object at 0x7f15953f2f20>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "agent = 2\n",
    "goal = 3\n",
    "dim = 50\n",
    "p = 0.3\n",
    "walls = 1\n",
    "path = 0\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.s = [i, j]\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        i, j = self.s\n",
    "        m, n = other.s\n",
    "        return State(i+m, j+n)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.s))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, State):\n",
    "            return self.s == other.s\n",
    "        return False\n",
    "    def __lt__(self, other):\n",
    "        # Compare the priorities of the states\n",
    "        return self.priority() < other.priority()\n",
    "    \n",
    "    def priority(self):\n",
    "        # Define the priority of the state based on your desired criteria\n",
    "        # For example, you can use the Manhattan distance to the goal state\n",
    "        goal_state = State(dim - 1, dim - 1)\n",
    "        return abs(self.s[0] - goal_state.s[0]) + abs(self.s[1] - goal_state.s[1])\n",
    "\n",
    "    def neighbors(self, maze):\n",
    "        neighbors = []\n",
    "        neighbors.append(self + State(-1, 0))  # Up\n",
    "        neighbors.append(self + State(1, 0))   # Down\n",
    "        neighbors.append(self + State(0, -1))  # Left\n",
    "        neighbors.append(self + State(0, 1))   # Right\n",
    "        valid_neighbors = []\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor.s[0] >= 0 and neighbor.s[0] < maze.shape[0] and \\\n",
    "               neighbor.s[1] >= 0 and neighbor.s[1] < maze.shape[1] and \\\n",
    "               maze[neighbor] != walls:\n",
    "                valid_neighbors.append(neighbor)\n",
    "        return valid_neighbors\n",
    "    \n",
    "    \n",
    "class Maze:\n",
    "    def __init__(self, dim, prob):\n",
    "        self.maze = {}\n",
    "        self.shape = (dim, dim)\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                self.maze[State(i, j)] = np.random.choice([path, walls], p = [1-prob, prob])\n",
    "        self.states = self.maze.keys()\n",
    "        \n",
    "        self.start_state = State(0,0)\n",
    "        self.goal_state = State(dim-1, dim-1)\n",
    "\n",
    "        self.maze[self.goal_state] = goal\n",
    "        self.maze[self.start_state] = agent\n",
    "    def __getitem__(self, state: State):\n",
    "        if isinstance(state, State):\n",
    "            return self.maze[state]\n",
    "        else:\n",
    "            try:\n",
    "                s = State(state[0], state[1])\n",
    "                return self.maze[s]\n",
    "            except:\n",
    "                print(\"error when calling the get_item of maze\")\n",
    "                exit(0)\n",
    "    def update(self, state, new_state):\n",
    "        if self.maze[new_state] != walls and self.maze[new_state] != goal:\n",
    "            self.maze[state] = path\n",
    "            self.maze[new_state] = agent\n",
    "\n",
    "    def a_star_search(self):\n",
    "        start_state = self.start_state\n",
    "        goal_state = self.goal_state\n",
    "        frontier = [(0, start_state)]  # Priority queue, starting with the start state\n",
    "        came_from = {}  # Dictionary to track the parent state for each visited state\n",
    "        cost_so_far = {}  # Dictionary to track the cost to reach each state\n",
    "        \n",
    "        came_from[start_state] = None\n",
    "        cost_so_far[start_state] = 0\n",
    "        \n",
    "        while frontier:\n",
    "            _, current_state = heapq.heappop(frontier)  # Pop the state with the lowest priority from the frontier\n",
    "            \n",
    "            if current_state == goal_state:\n",
    "                break\n",
    "            \n",
    "            for next_state in current_state.neighbors(self):\n",
    "                new_cost = cost_so_far[current_state] + 1  # Assuming a cost of 1 to move to the next state\n",
    "                if next_state not in cost_so_far or new_cost < cost_so_far[next_state]:\n",
    "                    cost_so_far[next_state] = new_cost\n",
    "                    priority = new_cost + heuristic(next_state, goal_state)\n",
    "                    heapq.heappush(frontier, (priority, next_state))\n",
    "                    came_from[next_state] = current_state\n",
    "        \n",
    "        # Reconstruct the path from the goal state to the start state\n",
    "        path = []\n",
    "        current_state = goal_state\n",
    "        while current_state != start_state:\n",
    "            path.append(current_state)\n",
    "            current_state = came_from[current_state]\n",
    "        path.append(start_state)\n",
    "        path.reverse()\n",
    "        \n",
    "        return path\n",
    "\n",
    "\n",
    "def heuristic(state, goal_state):\n",
    "    # Calculate the Manhattan distance between two states\n",
    "    return abs(state.s[0] - goal_state.s[0]) + abs(state.s[1] - goal_state.s[1])\n",
    "    \n",
    "def create_maze(dim = 10, p = 0.1):\n",
    "    #we create a maze with dimension dim x dim that have walls with probability p\n",
    "    maze = np.random.choice([0,1], size = (dim,dim), p = [1-p,p])\n",
    "    maze[0,0] = agent\n",
    "    maze[dim-1,dim-1] = goal\n",
    "    #the agent will start in position 0,0 and the goal is to reach position dim-1,dim-1\n",
    "    return maze.astype(\"int\")\n",
    "\n",
    "maze = Maze(dim, p)\n",
    "print(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "actions = [State(-1,0),State(1,0),State(0,-1),State(0,1), State(0, 0)] #up, down, left, right\n",
    "states = []\n",
    "for i in range(maze.shape[0]):\n",
    "    for j in range(maze.shape[1]):\n",
    "        states.append(State(i,j))\n",
    "\n",
    "def eval_state(maze, state, dim, action):\n",
    "    if action.s == [0, 0]: return -10\n",
    "    x, y = state.s\n",
    "    if x < 0 or x >= dim or y < 0 or y >= dim: return -10*dim\n",
    "    cell = maze[state]\n",
    "    if cell == 0: return -1\n",
    "    if cell == 1: return -10*dim\n",
    "    if cell == goal: return 10*dim\n",
    "    if cell == agent: return -dim\n",
    "\n",
    "print(eval_state(maze, State(dim-1, dim-1), dim, State(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def available_actions(state, actions, dim):\n",
    "    available = []\n",
    "    for action in actions:\n",
    "        new_state = state + action\n",
    "        x, y = new_state.s\n",
    "        if x >= 0 and x < dim and y >= 0 and y < dim: available.append(action)\n",
    "    \n",
    "    return available\n",
    "\n",
    "class ProposedActions:\n",
    "    def __init__(self, dim):\n",
    "        self.maze = maze\n",
    "        self.dict = {}\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                self.dict[State(i,j)] = available_actions(State(i, j), actions, dim)[0]\n",
    "    \n",
    "    def __getitem__(self, state):\n",
    "        if isinstance(state, State):\n",
    "            return self.dict[state]\n",
    "        else:\n",
    "            try:\n",
    "                s = State(state[0], state[1])\n",
    "                return self.dict[s]\n",
    "            except:\n",
    "                print(\"error when calling the get_item of maze\")\n",
    "                exit(0)\n",
    "    \n",
    "    \n",
    "proposed_actions = ProposedActions(dim)\n",
    "proposed_actions[0, 0].s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all the proposed states are in available actions\n",
    "for state in states:\n",
    "    for proposed in available_actions(state, actions, dim):\n",
    "        temp = state + proposed\n",
    "        temp = temp.s\n",
    "        if temp[0] < 0 or temp[0] >= dim or temp[1] < 0 or temp[1] >= dim:\n",
    "            print(state, temp, proposed)\n",
    "\n",
    "for state in states:\n",
    "    try:\n",
    "        proposed = proposed_actions[state]\n",
    "    except IndexError:\n",
    "        print(state)\n",
    "        break\n",
    "    new_state = state + proposed\n",
    "    temp = new_state.s\n",
    "    if temp[0] < 0 or temp[0] >= dim or temp[1] < 0 or temp[1] >= dim:\n",
    "        print(state, temp, proposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QTable:\n",
    "    def __init__(self, maze, actions, dim):\n",
    "        self.q_table = {}\n",
    "        for state in maze.states:\n",
    "            self.q_table[state] = {}\n",
    "            for available in available_actions(state, actions, dim):\n",
    "                self.q_table[state][available] = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        state, action = index\n",
    "        if action in available_actions(state, actions, dim):\n",
    "            return self.q_table[state][action]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def update(self, state, action, new_state, reward, alpha, gamma):\n",
    "        current_q_value = self.q_table[state][action]\n",
    "        max_future_q_value = max(self.q_table[new_state].values())\n",
    "\n",
    "        new_q_value = (1 - alpha) * current_q_value + alpha * (reward + gamma * max_future_q_value)\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "    def get_action(self, state, epsilon, actions, dim):\n",
    "        availables = available_actions(state, actions, dim)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(availables)\n",
    "        else:\n",
    "            action = max(self.q_table[state], key = self.q_table[state].get)\n",
    "\n",
    "        return action\n",
    "\n",
    "Q = QTable(maze, actions, dim)\n",
    "\n",
    "\n",
    "state = State(0,0)\n",
    "action = State(1, 0)\n",
    "Q[state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters\n",
    "lr = 0.05\n",
    "gamma = 0.9\n",
    "eps = 0.1 #exploration-exploration trade off, the higher the more exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def trip(maze, dim, actions, Q, lr = 0.1, gamma = 0.9, eps = 0.1):\n",
    "    player = State(0, 0)\n",
    "    iter = 100000\n",
    "    tot_reward = 0\n",
    "    path = []\n",
    "    path.append(player)\n",
    "    while maze[player] != goal and iter != 0:\n",
    "        iter -= 1\n",
    "        action = Q.get_action(player, eps, actions, dim)\n",
    "        new_state = player + action            \n",
    "        reward = eval_state(maze, new_state, dim, action)\n",
    "        Q.update(player, action, new_state, reward, lr, gamma)\n",
    "        player = new_state\n",
    "        path.append(player)\n",
    "        tot_reward += reward\n",
    "    \n",
    "    return player, tot_reward, path\n",
    "\n",
    "def train(maze, actions, dim, epochs = 300):\n",
    "    Q = QTable(maze, actions, dim)\n",
    "    finished = 0\n",
    "    best_reward = -1000000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        state, reward, _ = trip(maze, dim, actions, Q)\n",
    "        if reward == -100:\n",
    "            continue\n",
    "        \n",
    "        if state.s == [dim-1, dim-1]: finished += 1\n",
    "        if best_reward < reward: best_reward = reward\n",
    "    \n",
    "    return finished / epochs, best_reward, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/5000 [00:02<16:57,  4.90it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     maze\u001b[39m.\u001b[39ma_star_search()\n\u001b[0;32m----> 3\u001b[0m     finishing_rate, best, Q \u001b[39m=\u001b[39m train(maze, actions, dim, \u001b[39m5000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     finishing_rate, best\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(maze, actions, dim, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m best_reward \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1000000\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 26\u001b[0m     state, reward, _ \u001b[39m=\u001b[39m trip(maze, dim, actions, Q)\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m:\n\u001b[1;32m     28\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m, in \u001b[0;36mtrip\u001b[0;34m(maze, dim, actions, Q, lr, gamma, eps)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39miter\u001b[39m \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     11\u001b[0m action \u001b[39m=\u001b[39m Q\u001b[39m.\u001b[39mget_action(player, eps, actions, dim)\n\u001b[0;32m---> 12\u001b[0m new_state \u001b[39m=\u001b[39m player \u001b[39m+\u001b[39;49m action            \n\u001b[1;32m     13\u001b[0m reward \u001b[39m=\u001b[39m eval_state(maze, new_state, dim, action)\n\u001b[1;32m     14\u001b[0m Q\u001b[39m.\u001b[39mupdate(player, action, new_state, reward, lr, gamma)\n",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m, in \u001b[0;36mState.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     16\u001b[0m i, j \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms\n\u001b[1;32m     17\u001b[0m m, n \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39ms\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m State(i\u001b[39m+\u001b[39;49mm, j\u001b[39m+\u001b[39;49mn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    maze.a_star_search()\n",
    "    finishing_rate, best, Q = train(maze, actions, dim, 5000)\n",
    "    finishing_rate, best\n",
    "except KeyError:\n",
    "    print(\"The maze is impossible to resolve\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(maze, i, j, rl_player, a_player, history, history_a, A_star = False):\n",
    "    # Define colors\n",
    "    PATH_COLOR = (255, 255, 255)\n",
    "    WALL_COLOR = (0, 0, 0)\n",
    "    GOAL_COLOR = (255, 0, 0)\n",
    "    RL_AGENT_COLOR = (0, 0, 255)\n",
    "    A_AGENT_COLOR = (0, 255, 255)\n",
    "    TRACK_COLOR = (0, 255, 0)\n",
    "    TRACK_COLOR_A = (128, 128, 0)\n",
    "\n",
    "    val = maze[State(i, j)]\n",
    "    if [i, j] in history and [i, j] != rl_player.s and [i, j] != a_player.s:\n",
    "        return TRACK_COLOR   \n",
    "    if [i, j] in history_a and [i, j] != rl_player.s and [i, j] != a_player.s:\n",
    "        return TRACK_COLOR_A   \n",
    "    if val == agent or [i, j] == rl_player.s:\n",
    "        return RL_AGENT_COLOR\n",
    "    if val == agent or [i, j] == a_player.s:\n",
    "        return A_AGENT_COLOR\n",
    "    elif val == walls:\n",
    "        return WALL_COLOR\n",
    "    elif val == goal:\n",
    "        return GOAL_COLOR\n",
    "    else: return PATH_COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A* path len: 103, RL path len: 103\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from time import sleep\n",
    "\n",
    "# Initialize Pygame and create a window\n",
    "pygame.init()\n",
    "wait = 50\n",
    "screen_size = 1000\n",
    "screen = pygame.display.set_mode((screen_size, screen_size))\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Define grid parameters\n",
    "grid_size = dim\n",
    "cell_size = screen_size / grid_size\n",
    "\n",
    "# Main visualization loop\n",
    "running = True\n",
    "history = []\n",
    "history_a = []\n",
    "player = State(0, 0)\n",
    "eps = 0.0 #no exploration\n",
    "total_reward = 0\n",
    "player, reward, rl_path = trip(maze, dim, actions, Q, lr = 0.1, gamma = 0.9, eps = 0.0)\n",
    "a_path = maze.a_star_search()\n",
    "\n",
    "while running:\n",
    "    #pygame.time.wait(3000)\n",
    "    rl_player = rl_path[0]\n",
    "    a_player = a_path[0]\n",
    "    for state1, state2 in zip(rl_path[1:], a_path[1:]):\n",
    "        # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "        history.append(rl_player.s)\n",
    "        history_a.append(a_player.s)\n",
    "        rl_player = state1\n",
    "        a_player = state2\n",
    "        # Clear the screen\n",
    "        screen.fill(WHITE)\n",
    "\n",
    "        # Draw the grid\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                # Calculate the cell position and dimensions\n",
    "                x = col * cell_size\n",
    "                y = row * cell_size\n",
    "                points = [\n",
    "                    (x, y),\n",
    "                    (x + cell_size, y),\n",
    "                    (x + cell_size, y + cell_size),\n",
    "                    (x, y + cell_size),\n",
    "                ]\n",
    "                # Draw the cell\n",
    "                pygame.draw.polygon(screen, color(maze, row, col, rl_player, a_player, history, history_a), points)\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(wait)\n",
    "    \n",
    "    running = False\n",
    "\n",
    "# Clean up Pygame\n",
    "pygame.quit()\n",
    "\n",
    "print(f\"A* path len: {len(a_path)}, RL path len: {len(rl_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
