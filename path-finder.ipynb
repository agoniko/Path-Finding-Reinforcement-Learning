{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "agent = 2\n",
    "goal = 3\n",
    "dim = 10\n",
    "p = 0.2\n",
    "walls = 1\n",
    "path = 0\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.s = [i, j]\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        i, j = self.s\n",
    "        m, n = other.s\n",
    "        return State(i+m, j+n)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self.s))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, State):\n",
    "            return self.s == other.s\n",
    "        return False\n",
    "    def __lt__(self, other):\n",
    "        # Compare the priorities of the states\n",
    "        return self.priority() < other.priority()\n",
    "    \n",
    "    def priority(self):\n",
    "        # Define the priority of the state based on your desired criteria\n",
    "        # For example, you can use the Manhattan distance to the goal state\n",
    "        goal_state = State(dim - 1, dim - 1)\n",
    "        return abs(self.s[0] - goal_state.s[0]) + abs(self.s[1] - goal_state.s[1])\n",
    "\n",
    "    def neighbors(self, maze):\n",
    "        neighbors = []\n",
    "        neighbors.append(self + State(-1, 0))  # Up\n",
    "        neighbors.append(self + State(1, 0))   # Down\n",
    "        neighbors.append(self + State(0, -1))  # Left\n",
    "        neighbors.append(self + State(0, 1))   # Right\n",
    "        valid_neighbors = []\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor.s[0] >= 0 and neighbor.s[0] < maze.shape[0] and \\\n",
    "               neighbor.s[1] >= 0 and neighbor.s[1] < maze.shape[1] and \\\n",
    "               maze[neighbor] != walls:\n",
    "                valid_neighbors.append(neighbor)\n",
    "        return valid_neighbors\n",
    "    \n",
    "    \n",
    "class Maze:\n",
    "    def __init__(self, dim, prob):\n",
    "        self.maze = {}\n",
    "        self.shape = (dim, dim)\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                self.maze[State(i, j)] = np.random.choice([path, walls], p = [1-prob, prob])\n",
    "        self.states = self.maze.keys()\n",
    "        \n",
    "        self.start_state = State(0,0)\n",
    "        self.goal_state = State(dim-1, dim-1)\n",
    "\n",
    "        self.maze[self.goal_state] = goal\n",
    "        self.maze[self.start_state] = agent\n",
    "    def __getitem__(self, state: State):\n",
    "        if isinstance(state, State):\n",
    "            return self.maze[state]\n",
    "        else:\n",
    "            try:\n",
    "                s = State(state[0], state[1])\n",
    "                return self.maze[s]\n",
    "            except:\n",
    "                print(\"error when calling the get_item of maze\")\n",
    "                exit(0)\n",
    "    def update(self, state, new_state):\n",
    "        if self.maze[new_state] != walls and self.maze[new_state] != goal:\n",
    "            self.maze[state] = path\n",
    "            self.maze[new_state] = agent\n",
    "\n",
    "    def a_star_search(self):\n",
    "        start_state = self.start_state\n",
    "        goal_state = self.goal_state\n",
    "        frontier = [(0, start_state)]  # Priority queue, starting with the start state\n",
    "        came_from = {}  # Dictionary to track the parent state for each visited state\n",
    "        cost_so_far = {}  # Dictionary to track the cost to reach each state\n",
    "        \n",
    "        came_from[start_state] = None\n",
    "        cost_so_far[start_state] = 0\n",
    "        \n",
    "        while frontier:\n",
    "            _, current_state = heapq.heappop(frontier)  # Pop the state with the lowest priority from the frontier\n",
    "            \n",
    "            if current_state == goal_state:\n",
    "                break\n",
    "            \n",
    "            for next_state in current_state.neighbors(self):\n",
    "                new_cost = cost_so_far[current_state] + 1  # Assuming a cost of 1 to move to the next state\n",
    "                if next_state not in cost_so_far or new_cost < cost_so_far[next_state]:\n",
    "                    cost_so_far[next_state] = new_cost\n",
    "                    priority = new_cost + heuristic(next_state, goal_state)\n",
    "                    heapq.heappush(frontier, (priority, next_state))\n",
    "                    came_from[next_state] = current_state\n",
    "        \n",
    "        # Reconstruct the path from the goal state to the start state\n",
    "        path = []\n",
    "        current_state = goal_state\n",
    "        while current_state != start_state:\n",
    "            path.append(current_state)\n",
    "            current_state = came_from[current_state]\n",
    "        path.append(start_state)\n",
    "        path.reverse()\n",
    "        \n",
    "        return path\n",
    "\n",
    "\n",
    "def heuristic(state, goal_state):\n",
    "    # Calculate the Manhattan distance between two states\n",
    "    return abs(state.s[0] - goal_state.s[0]) + abs(state.s[1] - goal_state.s[1])\n",
    "    \n",
    "def create_maze(dim = 10, p = 0.1):\n",
    "    #we create a maze with dimension dim x dim that have walls with probability p\n",
    "    maze = np.random.choice([0,1], size = (dim,dim), p = [1-p,p])\n",
    "    maze[0,0] = agent\n",
    "    maze[dim-1,dim-1] = goal\n",
    "    #the agent will start in position 0,0 and the goal is to reach position dim-1,dim-1\n",
    "    return maze.astype(\"int\")\n",
    "\n",
    "maze = Maze(dim, p)\n",
    "print(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [State(-1,0),State(1,0),State(0,-1),State(0,1), State(0, 0)] #up, down, left, right\n",
    "states = []\n",
    "for i in range(maze.shape[0]):\n",
    "    for j in range(maze.shape[1]):\n",
    "        states.append(State(i,j))\n",
    "\n",
    "def eval_state(maze, state, dim, action):\n",
    "    if action.s == [0, 0]: return -10\n",
    "    x, y = state.s\n",
    "    if x < 0 or x >= dim or y < 0 or y >= dim: return -100\n",
    "    cell = maze[state]\n",
    "    if cell == 0: return -1\n",
    "    if cell == 1: return -100\n",
    "    if cell == goal: return 100\n",
    "    if cell == agent: return -10\n",
    "\n",
    "print(eval_state(maze, State(dim-1, dim-1), dim, State(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state, actions, dim):\n",
    "    available = []\n",
    "    for action in actions:\n",
    "        new_state = state + action\n",
    "        x, y = new_state.s\n",
    "        if x >= 0 and x < dim and y >= 0 and y < dim: available.append(action)\n",
    "    \n",
    "    return available\n",
    "\n",
    "class ProposedActions:\n",
    "    def __init__(self, dim):\n",
    "        self.maze = maze\n",
    "        self.dict = {}\n",
    "        for i in range(dim):\n",
    "            for j in range(dim):\n",
    "                self.dict[State(i,j)] = available_actions(State(i, j), actions, dim)[0]\n",
    "    \n",
    "    def __getitem__(self, state):\n",
    "        if isinstance(state, State):\n",
    "            return self.dict[state]\n",
    "        else:\n",
    "            try:\n",
    "                s = State(state[0], state[1])\n",
    "                return self.dict[s]\n",
    "            except:\n",
    "                print(\"error when calling the get_item of maze\")\n",
    "                exit(0)\n",
    "    \n",
    "    \n",
    "proposed_actions = ProposedActions(dim)\n",
    "proposed_actions[0, 0].s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all the proposed states are in available actions\n",
    "for state in states:\n",
    "    for proposed in available_actions(state, actions, dim):\n",
    "        temp = state + proposed\n",
    "        temp = temp.s\n",
    "        if temp[0] < 0 or temp[0] >= dim or temp[1] < 0 or temp[1] >= dim:\n",
    "            print(state, temp, proposed)\n",
    "\n",
    "for state in states:\n",
    "    try:\n",
    "        proposed = proposed_actions[state]\n",
    "    except IndexError:\n",
    "        print(state)\n",
    "        break\n",
    "    new_state = state + proposed\n",
    "    temp = new_state.s\n",
    "    if temp[0] < 0 or temp[0] >= dim or temp[1] < 0 or temp[1] >= dim:\n",
    "        print(state, temp, proposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, maze, actions, dim):\n",
    "        self.q_table = {}\n",
    "        for state in maze.states:\n",
    "            self.q_table[state] = {}\n",
    "            for available in available_actions(state, actions, dim):\n",
    "                self.q_table[state][available] = 0\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        state, action = index\n",
    "        if action in available_actions(state, actions, dim):\n",
    "            return self.q_table[state][action]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def update(self, state, action, new_state, reward, alpha, gamma):\n",
    "        current_q_value = self.q_table[state][action]\n",
    "        max_future_q_value = max(self.q_table[new_state].values())\n",
    "\n",
    "        new_q_value = (1 - alpha) * current_q_value + alpha * (reward + gamma * max_future_q_value)\n",
    "        self.q_table[state][action] = new_q_value\n",
    "\n",
    "    def get_action(self, state, epsilon, actions, dim):\n",
    "        availables = available_actions(state, actions, dim)\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(availables)\n",
    "        else:\n",
    "            action = max(self.q_table[state], key = self.q_table[state].get)\n",
    "\n",
    "        return action\n",
    "\n",
    "Q = QTable(maze, actions, dim)\n",
    "\n",
    "\n",
    "state = State(0,0)\n",
    "action = State(1, 0)\n",
    "Q[state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters\n",
    "lr = 0.1\n",
    "gamma = 0.9\n",
    "eps = 0.1 #exploration-exploration trade off, the higher the more exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def trip(maze, dim, actions, Q, lr = 0.1, gamma = 0.9, eps = 0.1):\n",
    "    player = State(0, 0)\n",
    "    iter = 10000\n",
    "    tot_reward = 0\n",
    "    path = []\n",
    "    path.append(player)\n",
    "    while maze[player] != goal and iter != 0:\n",
    "        iter -= 1\n",
    "        action = Q.get_action(player, eps, actions, dim)\n",
    "        new_state = player + action            \n",
    "        reward = eval_state(maze, new_state, dim, action)\n",
    "        Q.update(player, action, new_state, reward, lr, gamma)\n",
    "        player = new_state\n",
    "        path.append(player)\n",
    "        tot_reward += reward\n",
    "    \n",
    "    return player, tot_reward, path\n",
    "\n",
    "def train(maze, actions, dim, epochs = 300):\n",
    "    Q = QTable(maze, actions, dim)\n",
    "    finished = 0\n",
    "    best_reward = -1000000\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        state, reward, _ = trip(maze, dim, actions, Q)\n",
    "        if reward == -100:\n",
    "            continue\n",
    "        \n",
    "        if state.s == [dim-1, dim-1]: finished += 1\n",
    "        if best_reward < reward: best_reward = reward\n",
    "    \n",
    "    return finished / epochs, best_reward, Q\n",
    "\n",
    "finishing_rate, best, Q = train(maze, actions, dim)\n",
    "finishing_rate, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color(maze, i, j, player, history):\n",
    "    # Define colors\n",
    "    PATH_COLOR = (255, 255, 255)\n",
    "    WALL_COLOR = (0, 0, 0)\n",
    "    GOAL_COLOR = (255, 0, 0)\n",
    "    AGENT_COLOR = (0, 0, 255)\n",
    "    TRACK_COLOR = (0, 255, 0)\n",
    "\n",
    "    val = maze[State(i, j)]\n",
    "    if [i, j] in history and [i, j] != player.s:\n",
    "        return TRACK_COLOR    \n",
    "    if val == agent or [i, j] == player.s:\n",
    "        return AGENT_COLOR\n",
    "    elif val == walls:\n",
    "        return WALL_COLOR\n",
    "    elif val == goal:\n",
    "        return GOAL_COLOR\n",
    "    else: return PATH_COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from time import sleep\n",
    "\n",
    "# Initialize Pygame and create a window\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((1000, 1000))\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Define grid parameters\n",
    "grid_size = dim\n",
    "cell_size = 100\n",
    "\n",
    "# Main visualization loop\n",
    "running = True\n",
    "history = []\n",
    "player = State(0, 0)\n",
    "eps = 0.0 #no exploration\n",
    "total_reward = 0\n",
    "player, reward, rl_path = trip(maze, dim, actions, Q, lr = 0.1, gamma = 0.9, eps = 0.0)\n",
    "a_path = maze.a_star_search()\n",
    "\n",
    "while running:\n",
    "    player = rl_path[0]\n",
    "    for state in rl_path[1:]:\n",
    "            # Handle events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        history.append(player.s)\n",
    "        player = state\n",
    "        # Clear the screen\n",
    "        screen.fill(WHITE)\n",
    "\n",
    "        # Draw the grid\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                # Calculate the cell position and dimensions\n",
    "                x = col * cell_size\n",
    "                y = row * cell_size\n",
    "                points = [\n",
    "                    (x, y),\n",
    "                    (x + cell_size, y),\n",
    "                    (x + cell_size, y + cell_size),\n",
    "                    (x, y + cell_size),\n",
    "                ]\n",
    "                # Draw the cell\n",
    "                pygame.draw.polygon(screen, color(maze, row, col, player, history), points)\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        pygame.time.wait(500)\n",
    "\n",
    "# Clean up Pygame\n",
    "pygame.quit()\n",
    "\n",
    "print(f\"Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = maze.a_star_search()\n",
    "print(-len(path)+100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
